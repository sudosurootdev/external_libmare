<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2010-2013 Qualcomm Technologies, Inc. All rights reserved. 
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 header.html - Header file used to customize the headers on each HTML page for the API 
               Interface Specification documents.
 Updated:  03/08/13 LB Updated to support Doxygen 1.8.3.1.
 Updated:  10/25/12 LB Added company name changes.
 New file: 10/15/12 LB Created to support Rev E Alpha process using Doxygen 1.8.2 -->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Multicore Asynchronous Runtime Environment Parallel Processing Tutorial</title><!--END !PROJECT_NAME-->
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" />
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Multicore Asynchronous Runtime Environment
   &#160;<span id="projectnumber">HT80-NG608-1 E</span>
   </div>
  </td>
  <!--END !PROJECT_NAME-->
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1-QCOM -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Home</span></a></li>
      <li><a href="revision_history.html"><span>Revision&#160;History</span></a></li>
      <li><a href="introduction.html"><span>Introduction</span></a></li>
      <li><a href="usergroup0.html"><span>Getting&#160;Started</span></a></li>
      <li><a href="usergroup1.html"><span>User&#160;Guide</span></a></li>
      <li><a href="usergroup3.html"><span>Tutorials</span></a></li>
      <li><a href="usergroup4.html"><span>Reference&#160;Manual</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('pp.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Parallel Processing Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="abstract"></a>
Abstract</h1>
<p>In this tutorial we introduce general principles of parallel programming with an emphasis on task-based parallel programming models. We first introduce scaling as a metric of evaluating the potential speedup that an algorithm can obtain. Then we discuss different parallel programming paradigms and a number of optimizations for parallel code. We illustrate our discussion with examples using the MARE programming model.</p>
<h1><a class="anchor" id="sec_pp_amdahl"></a>
Parallel Speedups</h1>
<p>Amdahl<a class="el" href="citelist.html#CITEREF_Amdahl:Law:AFIPS:1967">[2]</a> put forward an argument that the maximum speedup that can be obtained by a parallel algorithm is bounded by the serial fraction of the program. Intuitively, even if we could execute the parallel fraction infinitely fast (zero time), the serial fraction will determine the total execution time. This argument, commonly known as <em>Amdahl's Law</em>, can be summarized by the following equation, when considering \(N\) parallel processors:</p>
<p class="formulaDsp">
\[Parallel Speedup = \frac{s + p}{s + p/N} = \frac{1}{s+p/N},\]
</p>
<p> where \(s + p = 1\), representing the serial and parallel fractions of the program, respectively. Using Amdahl's law, the speedup that can be obtained with 8 processors as a function of the serial fraction is illustrated in Figure <a class="el" href="pp.html#Amdahl">Amdahl</a>.</p>
<p><a class="anchor" id="Amdahl"></a><br/>
  <div style="text-align:center;"><img src="amdahl_HTML.png" alt="Theoretical speedup on 8 processors using Amdahl's law" width="80%" height="80%"><br/><br/><b>Theoretical speedup on 8 processors using Amdahl's law</b></div><br/> <p>Note that even if the serial fraction is only 10%, the maximum theoretical speedup achievable is 4.58. In practice, however, hardware architecture characteristics, such as caching, allow programmers to obtain much better performance from multicore systems. Amdahl's law expresses performance increase for constant problem size (<em> strong scaling</em>). Gustafson<a class="el" href="citelist.html#CITEREF_Gustafson:Amdahl:CACM:1988">[11]</a> demonstrates that parallel processing can be used to perform more work in the same amount of time by increasing the problem size, thus improving scalability. We call this technique <em> weak scaling</em>. Architectural artifacts<a class="el" href="citelist.html#CITEREF_Hill:Amdahl:Computer:2008">[13]</a> also play an important role; additional processors come with additional cache and memory resources, often enabling applications to obtain super-linear speedup. We shall discuss a number of optimizations that take advantage of architectural features in Section <a class="el" href="pp.html#sec_pp_opts">Optimizations</a>.</p>
<h1><a class="anchor" id="sec_pp_paradigms"></a>
Parallel Programming Paradigms</h1>
<p>When discussing parallel programming, practitioners classify the different types of parallelism loosely following machine organizations<a class="el" href="citelist.html#CITEREF_Flynn:TOC:1972">[8]</a> : </p>
<ul>
<li><b>Data parallelism (SIMD):</b> SIMD machines include vector units, array processors, and GPUs. In this model, the program is executing the same code on different data elements. Data parallel algorithms are typically expressed as operations on a multi-dimensional array. Control flow is uniform; however, operations on certain elements may be masked out. Image processing algorithms are prototypical for data parallelism. In the current version of MARE, one can exploit data parallelism by using vector intrinsics<a class="el" href="citelist.html#CITEREF_NEONintrinsics">[18]</a> to target the NEON units, or by calling OpenGL functions to execute on the GPU. Future versions of MARE will support SIMD compute on the GPU as part of the programming model. In the code example below we show a simple example of scalar vector multiply (SAXPY) using MARE tasks and vector operations.</li>
</ul>
<p><a class="anchor" id="SIMD"></a></p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> saxpy(<span class="keywordtype">float</span>* y, <span class="keywordtype">float</span> a, <span class="keywordtype">float</span>* x, <span class="keywordtype">int</span> n) { <span class="comment">// Y = a * X</span></div>
<div class="line">  <span class="keywordtype">int</span> i;</div>
<div class="line"></div>
<div class="line">  assert(n%4 == 0); <span class="comment">// for simplicity we multiply only vector sizes multiple of 4</span></div>
<div class="line"></div>
<div class="line">  <span class="keyword">auto</span> g = <a class="code" href="group__groups__creation.html#ga164f9c8b577069b2d8b1634cde459b1e">create_group</a>(<span class="stringliteral">&quot;saxpy&quot;</span>);</div>
<div class="line">  float32x4_t av = vmovq_n_f32(a);    <span class="comment">// initialize all lanes to a</span></div>
<div class="line">  <span class="keywordflow">for</span> (i = 0; i &lt; n; i+=4) {</div>
<div class="line">    <a class="code" href="group__sdf__doc.html#gaf6d88ec9965da7eaa9da60ab7c27041b">launch</a>(g, [&amp;av, x, y, i] {        <span class="comment">// create a task for each vector op</span></div>
<div class="line">      float32x4_t xv, yv;</div>
<div class="line">      memcpy(&amp;xv.res, x[i], 4*<span class="keyword">sizeof</span>(<span class="keywordtype">float</span>)); <span class="comment">// initialize the vector regs</span></div>
<div class="line">      memcpy(&amp;yv.res, y[i], 4*<span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div>
<div class="line">      vmlaq_f32(yv, av, xv);          <span class="comment">//     y[i] += a * x[i];</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <a class="code" href="group__groups__waiting.html#gaa2a0ab6d7f24ae0840b581bfa8e94fea">wait_for</a>(g);                        <span class="comment">// wait for all tasks to complete</span></div>
<div class="line">}</div>
</div><!-- fragment --><ul>
<li><b>Task parallelism (MIMD):</b> MIMD machines are <em>multiprocessors</em>. In this model, different hardware execution contexts (e.g., threads, cores, processors) execute different code on different data elements. Tasks are either independent or cooperate on processing over a shared data structure. Thus, tasks may have control or data dependencies. The irregular structure of the computation complicates handling of dependencies and synchronization. Typical applications include: physical simulations, computer simulations, browsers<a class="el" href="citelist.html#CITEREF_Cascaval:Zoomm:PPoPP:2013">[6]</a>, etc. Task parallelism is supported in MARE and examples are provided in the MARE user's manual<a class="el" href="citelist.html#CITEREF_mare:manual">[17]</a>.</li>
</ul>
<ul>
<li><b>Braided parallelism:</b> Modern machines combine CPUs and GPUs for heterogeneous general purpose computation. Recently there has been a significant increase in GPGPU (general purpose GPU) programming. The braided parallelism model combines task parallel computation with data parallel execution<a class="el" href="citelist.html#CITEREF_Gaster:Computer:2012">[9]</a>. This unified model is used to dynamically exploit data parallelism on SIMD units and GPUs from within concurrent tasks executing on the MIMD units. Examples include gaming applications, which have many concurrent tasks (physics, AI, UI) that are composed from data-parallel computations, such as particle simulations, image processing and rendering.</li>
</ul>
<ul>
<li><b>Pipeline parallelism or Streaming:</b> Distributed processor architectures are composed of separate hardware contexts, such as Cell<a class="el" href="citelist.html#CITEREF_IBM:Cell">[15]</a>, and designed to exploit small working sets and/or algorithms with little locality. Computation organized as pipeline stages allows code to reside on each unit and the data is streamed across. The pattern of dependencies is fixed, simplyfying the parallel execution. Tuning and balancing is complicated by the predefined computation structure, and may require significant reorganization. Many algorithms are amenable to pipeline parallelism when partitioned. Examples include computer vision algorithms, search, etc.</li>
</ul>
<p>MARE uses a task based programming model (we plan to support braided parallelism in future iterations). that encourages programmers to think in terms of abstractions that express concurrency as independent units of work. Dependencies are allowed and MARE makes it easy to express these as dynamic task graphs. The MARE programming model is embedded in the C++ language and supported by a runtime library. The entire programming model is implemented as library APIs, therefore can run on any compiler that supports the C++11 standard<a class="el" href="citelist.html#CITEREF_Cpp11std">[5]</a>. This design allows us to take advantage of some of the concurrency abstractions defined in the C++11 standard, such as memory model and synchronization.</p>
<p>To parallelize a computation using MARE, programmers need to address four aspects:</p>
<ul>
<li><b>Computation:</b> Computation is expressed as a partially ordered task graph. Each task is an independent unit of work. In MARE tasks are created using the <code>mare::create_task</code> method. Dependencies between tasks ensure the proper ordering of computation. In MARE we decouple task creation from task spawning (or launching) to allow the programmer to set up all the necessary dependencies. Dependencies are set up using the <code>mare::after</code> method or the <code> operator &lt;&lt; </code>). Once a task is launched (using <code>mare::launch</code>) the MARE runtime takes control of the task and it executes it whenever all its dependencies are satisfied and resources are available. </li>
<li><b>Memory:</b> MARE is supported on shared memory systems. The entire address space is visible to all tasks, following the C++11 memory model. The programmer is responsible for using appropriate synchronization primitives to protect potentially concurrent memory accesses to the same location: C++11 mutexes and locks (e.g. <code>std::mutex</code>, <code>std::unique_lock</code>), and atomic variables and operations<a class="el" href="citelist.html#CITEREF_Cpp11std">[5]</a>,<a class="el" href="citelist.html#CITEREF_Gregoire:Cpp11">[10]</a>. </li>
<li><b>Synchronization:</b> Besides memory synchronization, MARE also supports task synchronization. Programmers setup dependencies between tasks, which ensure a partial ordering of the computation. In addition, tasks can wait for launched computation to finish using the <code>mare::wait_for</code> method, either individually or as a group. Task synchronization can also be implemented using C++11 condition variables (<code>std::condition_variable</code>).</li>
</ul>
<h1><a class="anchor" id="sec_pp_patterns"></a>
Parallel Programming Patterns</h1>
<p>Built upon the basic APIs to create and manage tasks, MARE supports a number of higher level parallel programming patterns. The APIs are described<a class="el" href="citelist.html#CITEREF_mare:manual">[17]</a>, and summarized here.</p>
<p>Parallel iteration is the most common pattern of expressing that the elements of a collection can be processed in parallel. MARE provides two version of this pattern <code>mare::pfor_each</code>, which waits at the end of the iteration for all tasks to complete, and <code>mare::pfor_each_async</code> which continues execution, while tasks may still be processing elements.</p>
<p>The method <code>mare::pscan</code> performs an inplace parallel prefix operation<a class="el" href="citelist.html#CITEREF_Hillis:CACM:1986">[14]</a> for all elements of a collection. The method <code>mare::ptransform</code> performs a map operation on all elements of the collection, returning a new collection.</p>
<p>Future versions of MARE will provide more patterns, however, programmers are encouraged to define their own.</p>
<h1><a class="anchor" id="sec_pp_opts"></a>
Optimizations</h1>
<p>Beside algorithmic decomposition of work and data, a parallel program requires tuning to a specific platform to achieve optimal performance. As a general rule, the following process should be used:</p>
<ul>
<li><b>Serial tuning:</b> The code executed by each task should be optimized using classical optimization techniques: loop optimizations, strength reduction, and cache locality optimizations. </li>
<li><b>Synchronization tuning:</b> Coordinating parallel execution is typically considered overhead &ndash; the program executes additional instructions that are not necessarily part of the effective work. Such overhead includes serialization in critical sections, waiting for dependencies to be satisfied and/or condition variables to be signaled, etc. A well-tuned parallel program spends most of its time executing work as opposed to managing work. However, it may be necessary to replicate computation in order to minimize synchronization. </li>
<li><b>Parallel efficiency:</b> A parallel execution is optimal when all the execution units are equally busy, doing minimal redundant work. Therefore it is important to balance the computation across all processors. This can be achieved by a combination of algorithmic decomposition &mdash; finer grain tasks allow better load balancing, and taking into account architectural characteristics, such as resource sharing and overhead of spawning tasks &mdash; coarser grain tasks typically incur less overhead.</li>
</ul>
<p>In the next sections we touch briefly on some of these topics.</p>
<h2><a class="anchor" id="sec_pp_cache_locality"></a>
Cache locality</h2>
<p>There are two types of memory reference locality, <em>temporal locality</em> in which program references to a given memory address are clustered together in time, and <em>spatial locality</em>, where program references to neighboring addresses are clustered in time<a class="el" href="citelist.html#CITEREF_HenPat">[12]</a>. Caches transparently take advantage of both types of locality: replacement policies exploit temporal locality, while wide cache lines and prefetching techniques exploit spatial locality. Moreover, current architectures provide several levels of caches, with different sharing patterns. For example, level one caches (L1) are typically split between instructions and data, and are private to a core (shared by the hyperthreads in the case of an SMT architecture), and level two caches (L2) are shared by multiple cores.</p>
<p>Despite programmers not having direct control over caching, code and data can be structured to improves the locality of reference, and thus making effective use of cache mechanisms<a class="el" href="citelist.html#CITEREF_Rogers:Decomp:1989">[20]</a>. In a multicore system caches are a shared resource, so programmers should consider the following:</p>
<ul>
<li><b>Consistency:</b> Most multicore shared memory systems provide hardware coherency<a class="el" href="citelist.html#CITEREF_HenPat">[12]</a>. However, architectures implement different consistency models<a class="el" href="citelist.html#CITEREF_Adve:consistency:IEEE:95">[1]</a>, thereby affecting the way shared memory updates are visible to different threads. In particular, the ARM architecture defines a weak memory consistency model. The C++11 standard defines primitives to enforce the ordering of memory operations for all atomic accesses. The expert programmer can exploit non-sequential consistent orderings to obtain better performance on such systems. </li>
<li><b>False sharing:</b> False sharing<a class="el" href="citelist.html#CITEREF_Torrellas:FalseSharing:ICPP:1990">[21]</a> arises when independent data items used by two tasks executing concurrently on two different cores are co-located in the same cache line. Because the unit of coherence is the cache line, if the items are accessed by both tasks, the line will be forced by the coherence protocol to bounce between caches. False sharing can be avoided by separating data items accessed by different concurrent tasks into separate cache lines, using techniques such as padding<a class="el" href="citelist.html#CITEREF_RiveraTseng:PLDI98">[19]</a> and/or allocation to cache line boundaries. To improve locality of reference and limit memory fragmentation, programmers should group data items accessed by a single task as close as possible, preferrably in contiguous blocks of memory addresses. </li>
<li><b>Cache interference:</b> In serial applications cache optimizations are tuned to the entire cache. However, in a parallel application, caches are shared by execution units. A carefully tuned parallel program should maximize the utilization of the cache, by ensuring reuse of true shared data. For example, by maintaining a single copy of read-only shared data, and referencing it simultaneously, one will exploit temporal locality and minimize the amount of cache used. To minimize contention and interference, the working set sizes of the tasks should fit in the cache. Tiling and cache blocking<a class="el" href="citelist.html#CITEREF_McKinleyTiling">[7]</a>,<a class="el" href="citelist.html#CITEREF_WolfeTiling">[22]</a> parameters must be tuned considering the capacity when the caches are shared.</li>
</ul>
<p>Many other cache locality optimizations are described in the literature.</p>
<h2><a class="anchor" id="sec_pp_wait"></a>
Minimizing wait time and synchronization</h2>
<p>As mentioned, efficient parallel execution implies balanced execution of non-reduntant work on all computing units, while minimizing management overhead. We have shown how the fraction of serial execution (Amdahl's law) limits the efectiveness of the parallel application in Section <a class="el" href="pp.html#sec_pp_amdahl">Parallel Speedups</a>. Therefore, programmers should carefully consider different factors that serialize execution, including:</p>
<ul>
<li><b>Avoid waiting for single tasks:</b> Long chains of dependencies, and/or often waiting for the results of single tasks, limits the level of parallelism available in applications<a class="el" href="citelist.html#CITEREF_Kulkarni:PPoPP:2009">[16]</a>. MARE groups can be used to wait for sets of tasks, thus potentially minimizing the overall amount of stalling. </li>
<li><b>Data synchronization:</b> Synchronizing shared memory accesses may introduce considerable serialization or cache conflict overhead. Such overhead can be reduced by the following optimizations: <ul>
<li>
Privatize data<a class="el" href="citelist.html#CITEREF_Barton:lcpc:2006">[3]</a> &ndash; mutually exclusive partitioning of shared data. For example, partitioning an image into tiles, where each task works on a different tile. In cases where the partitioning is not obvious, programmers can copy shared data into private buffers, work on the private data, and then synchronize changes to the shared copy. Parallel reductions, and parallel gather and scatter operations are helpful in reshaping the private and shared data formats. </li>
<li>
Avoid large critical sections &ndash; Since critical sections guarantee mutual exclusion, they serialize the execution of tasks that are accessing these areas. Minimizing the time spent in critical sections, in particular when they are highly contended will reduce the synchronization overhead. </li>
<li>
Use atomic operations &ndash; the appropriate memory ordering further reduces the synchronization overhead and relies on hardware capabilities for efficient shared data accesses. </li>
</ul>
</li>
</ul>
<p>MARE encourages an asynchronous programming style, in which fine- grained tasks are placed in a dependence graph, and thus minimizes the need for waits. By contrast, fork-join models spawn a large set of work which needs to complete before the control flows from the join. Asynchronous concurrency is also preferable in the case of heterogeneous computing since resources need not be blocked waiting for an off-load device to complete the work.</p>
<h2><a class="anchor" id="sec_pp_lb"></a>
Load balancing</h2>
<p>Serialization is one of the potential pitfalls of parallel programming. Another is the under-utilization of all compute units. If the parallel computation is unbalanced, some processors will be idle, thereby cutting into the potential performance gains. To avoid such scenarios, programmers should pay attention to balancing the work. This can be achieved in several ways, the most popular being:</p>
<ul>
<li><b>Tuning the task granularity:</b> Task granularity represents the amount of work in a task. Ideally, if the amount of work is known, one can balance the computation manually. However, this is not the case for irregular applications, in which case, overdecomposition and relying on the MARE runtime dynamic scheduling is a better option. Task granularity also plays an important role in managing the overhead. As task granularity decreases, the overhead of managing the parallel execution becomes a larger fraction of the total time. Therefore, coarser tasks are preferred to minimize the overhead. This is an important balance that the programmers need to weigh. MARE makes it easy to explore these trade-offs by providing a set of flexible APIs to create tasks. </li>
<li><b>Overdecomposition:</b> Overdecomposition is the mechanism by which programmers ensure there is enough parallel work in the system, so that the runtime always has work to schedule. Overdecomposition is defined as creating more tasks than the number of computation units available, such that if a task blocks or waits for dependencies to be satisfied, other independent tasks continue to make progress. The more independent tasks are provided, the better the load balancing that can be achieved. Of course, one needs to take into consideration the task granularity and manage the overhead.</li>
</ul>
<h1><a class="anchor" id="sec_pp_concl"></a>
Conclusions</h1>
<p>Parallel programming is fun and intellectually challenging. There are many factors that come into play when building a parallel application, which may not be obvious. The techniques described in this tutorial will help you reach the main goal of parallel programming &mdash; speeding up the execution of the application. MARE is designed to ease this task and provide abstractions that make it convenient to express parallel computation. The hard work of creating a parallel algorithm remains; however, MARE and these techniques will help encoding these algorithms into an efficient solution. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2012-2013 Qualcomm Technologies, Inc. All rights reserved. 
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 footer.html - footer file used to customize the footers on each HTML page for the API 
               Interface Specification documents.
 Updated:  11/05/13 LB Removed confidential, distribution, and export statement.
 Updated:  09/20/13 LB Updated to support mare project's User Guide and API doc.
 Updated:  04/18/13 LB Added Documentation and Interface Specification in place of hard-coding doc dcn. Deleted original
                       use of Documentation and Interface Specification to display text in the left portion of the footer.
 Updated:  03/08/13 LB Updated to support Doxygen 1.8.3.1
 Updated:  01/18/13 LB Replaced qualcomm_logo.gif to QTI_Logo.png
 Updated:  10/25/12 LB Added company name changes.
 New file: 10/15/12 LB Created to support the Rev E Alpha Doxygen-to-HTML-PDF process. -->
<!-- start footer part -->
<!-- 7/12/12 JG added div class defined in css to include warning statement on all pages -->
<!--[if lte IE 8]><div class="warningmsgie8"><![endif]-->
<!--[if(gte IE 9)|!(IE)]><!--><div class="warningmsg"><!--<![endif]-->
</div>
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
		<p align="right">HT80-NG608-1 E<br />
        <img class="footer"  alt="QTI Logo" src="qti_logo_HTML.png" />
		</p>
		</li>
  </ul>
</div>
</body>
</html>
